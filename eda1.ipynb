{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD9d8KYyw98DkMaQlCJuTz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranil046/4-febasian/blob/main/eda1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3abr2cksNPWb"
      },
      "outputs": [],
      "source": [
        "1:predicting the quality of wine.\n",
        "ChatGPT said:\n",
        "The Wine Quality Dataset, commonly used in machine learning and data analysis,\n",
        "contains physicochemical attributes of wine and their corresponding quality\n",
        "ratings (typically on a scale of 0–10, though most values fall between 3–9).\n",
        "There are two versions of the dataset: red wine and white wine, both with the\n",
        "same features.\n",
        "\n",
        "2:The Wine Quality Dataset (Red and White), as provided by the UCI Machine\n",
        "Learning Repository, does not contain missing values by default. However,\n",
        "in practice or real-world scenarios, missing data may occur due to:\n",
        "\n",
        "Data collection errors\n",
        "\n",
        "Sensor failures\n",
        "\n",
        "Manual entry issues\n",
        "\n",
        "So, let’s discuss how to handle missing data if it existed, especially\n",
        "during feature engineering.\n",
        "\n",
        "3:Descriptive statistics: Mean, median, standard deviation of exam scores.\n",
        "\n",
        "Visualization:\n",
        "\n",
        "Histograms of exam scores.\n",
        "\n",
        "Box plots for categorical features (e.g., gender vs. score).\n",
        "\n",
        "Heatmaps for correlation between numeric features.\n",
        "\n",
        "4:Feature engineering is the process of transforming raw data into meaningful\n",
        "features that improve the performance of machine learning models. In the contex\n",
        "t of a Student Performance Dataset (e.g., from Kaggle or UCI), the goal is to\n",
        "prepare and select variables that most effectively predict student outcomes like\n",
        " exam scores or pass/fail.\n",
        "\n",
        "5:import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import shapiro, boxcox, skew\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"winequality-red.csv\")  # or winequality-white.csv\n",
        "# Plot histograms for all features\n",
        "df.hist(bins=30, figsize=(15, 12), color='skyblue', edgecolor='black')\n",
        "plt.suptitle(\"Feature Distributions\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "6:import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"winequality-red.csv\")  # or winequality-white.csv\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\"quality\", axis=1)  # Keep only numeric input features\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Cumulative explained variance\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
        "plt.axhline(y=0.90, color='r', linestyle='--')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA - Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "# Find number of components to explain at least 90% variance\n",
        "num_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
        "print(f\"Minimum number of principal components to explain 90% variance: {num_components_90}\")\n"
      ]
    }
  ]
}