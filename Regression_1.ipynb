{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYQMefe9RZiXQ86tGufKrt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranil046/4-febasian/blob/main/Regression_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwqBKp8DU1gc"
      },
      "outputs": [],
      "source": [
        "1:Simple linear regression models the relationship between one independent\n",
        "variable (predictor) and one dependent variable (outcome) using a straight line.\n",
        "Multiple linear regression models the relationship between two or more\n",
        "independent variables and one dependent variable.\n",
        "\n",
        "2:import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "\n",
        "# Suppose X is your independent variables DataFrame, y is dependent variable\n",
        "# Fit a linear regression model\n",
        "X = sm.add_constant(X)  # add intercept\n",
        "model = sm.OLS(y, X).fit()\n",
        "residuals = model.resid\n",
        "fitted = model.fittedvalues\n",
        "\n",
        "# 1. Linearity & Homoscedasticity\n",
        "plt.scatter(fitted, residuals)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Fitted values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Fitted values')\n",
        "plt.show()\n",
        "\n",
        "# 2. Normality of residuals\n",
        "sm.qqplot(residuals, line='45')\n",
        "plt.title('Q-Q plot of residuals')\n",
        "plt.show()\n",
        "\n",
        "# 3. Multicollinearity (only for multiple regression)\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['feature'] = X.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)\n",
        "\n",
        "# 4. Durbin-Watson test for independence\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "dw = durbin_watson(residuals)\n",
        "print(f'Durbin-Watson statistic: {dw:.2f}')\n",
        "\n",
        "\n",
        "3:The intercept is the predicted value of the dependent variable when all\n",
        "independent variables are zero.\n",
        "\n",
        "4:Imagine training a linear regression model where the goal is to find the best\n",
        "slope and intercept that minimize the mean squared error between predicted and\n",
        "actual values.\n",
        "\n",
        "Start with random slope and intercept.\n",
        "\n",
        "Compute the gradient of the error.\n",
        "\n",
        "Adjust the slope and intercept slightly against the gradient.\n",
        "\n",
        "Repeat until the error is minimal.\n",
        "\n",
        "5:Simple linear regression focuses on the relationship between one predictor\n",
        "and an outcome.\n",
        "\n",
        "Multiple linear regression includes multiple predictors, allowing for a more\n",
        "comprehensive model of factors influencing the outcome.\n",
        "\n",
        "6:Multicollinearity occurs when two or more independent variables in a multiple\n",
        "linear regression model are highly correlated with each other.\n",
        "\n",
        "This means these predictors provide redundant information about the outcome\n",
        "\n",
        " because they move together.\n",
        "\n",
        "7:Linear regression: Predict sales based on advertising spend, assuming sales\n",
        "increase linearly as spend increases.\n",
        "\n",
        "Polynomial regression: Predict sales based on advertising spend, but allowing\n",
        "for diminishing returns or acceleration (e.g., sales increase slowly at first,\n",
        "                                         then faster, then plateau).\n",
        "8:Models Nonlinear Relationships:\n",
        "Unlike linear regression, polynomial regression can capture curved, nonlinear\n",
        "patterns in data.\n",
        "\n",
        "More Flexible Fit:\n",
        "Can fit complex relationships better by adjusting the degree of the polynomial.\n",
        "\n",
        "Simple Extension of Linear Regression:\n",
        "Itâ€™s still a linear model in terms of coefficients, so many linear regression\n",
        "tools and diagnostics apply.\n"
      ]
    }
  ]
}