{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeiViQzQ2ikEjmARt86/uR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranil046/4-febasian/blob/main/ML2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf6LlrnCZFiO"
      },
      "outputs": [],
      "source": [
        "1:Definition: Overfitting occurs when a machine learning model learns not\n",
        "only the underlying patterns in the training data but also the noise and\n",
        "random fluctuations. As a result, it performs well on the training data\n",
        "but poorly on new, unseen data.\n",
        "\n",
        "Consequences:\n",
        "\n",
        "High accuracy on training data.\n",
        "\n",
        "Poor generalization to test or real-world data.\n",
        "\n",
        "Unreliable predictions on unseen inputs.\n",
        "\n",
        "Mitigation Techniques:\n",
        "\n",
        "Use simpler models with fewer parameters.\n",
        "\n",
        "Employ regularization methods (e.g., L1 or L2 regularization).\n",
        "\n",
        "Use more training data.\n",
        "\n",
        "Apply dropout (in neural networks).\n",
        "\n",
        "Use cross-validation to tune model parameters.\n",
        "\n",
        "Early stopping during training.\n",
        "\n",
        "Underfitting\n",
        "Definition: Underfitting happens when a model is too simple to capture the\n",
        "underlying structure of the data. It fails to perform well on both training and\n",
        "test data.\n",
        "\n",
        "Consequences:\n",
        "\n",
        "Low accuracy on training data and test data.\n",
        "\n",
        "Misses important patterns in the data.\n",
        "\n",
        "Mitigation Techniques:\n",
        "\n",
        "Use more complex models (e.g., adding more features or using deeper neural\n",
        "                         networks).\n",
        "\n",
        "Increase training time or reduce regularization.\n",
        "\n",
        "Improve feature engineering or input representation.\n",
        "\n",
        "Remove noise filtering if too aggressive.\n",
        "\n",
        "2:Simplify the Model:\n",
        "\n",
        "Use models with fewer parameters or less complexity (e.g., shallower trees or\n",
        "                                                     fewer layers in neural\n",
        "                                                     networks).\n",
        "\n",
        "Regularization:\n",
        "\n",
        "Apply techniques like L1 (Lasso) or L2 (Ridge) regularization, which add\n",
        "penalties to large weights and discourage over-complex models.\n",
        "\n",
        "Use More Training Data:\n",
        "\n",
        "More diverse and representative training data helps the model generalize better.\n",
        "\n",
        "Cross-Validation:\n",
        "\n",
        "Use techniques like k-fold cross-validation to ensure the model performs well\n",
        "across different subsets of the data.\n",
        "\n",
        "Early Stopping:\n",
        "\n",
        "In iterative training methods (like neural networks), stop training when\n",
        "performance on validation data starts to degrade.\n",
        "\n",
        "3:Underfitting occurs when a machine learning model is too simple to capture the\n",
        " underlying patterns in the data. It fails to learn the complexities and\n",
        " relationships in the data, leading to poor performance on both the training and\n",
        "  test datasets.\n",
        "\n",
        "Consequences of Underfitting:\n",
        "Low Accuracy: The model performs poorly on both training data and unseen test\n",
        "data.\n",
        "\n",
        "Missed Patterns: The model fails to capture important patterns and trends in the\n",
        " data.\n",
        "\n",
        "High Bias: Underfitting is often associated with high bias, where the model\n",
        "makes strong assumptions about the data, leading to inaccurate predictions.\n",
        "\n",
        "Scenarios Where Underfitting Can Occur:\n",
        "Using an Oversimplified Model:\n",
        "\n",
        "Example: Trying to fit a linear model (like linear regression) to a non-linear\n",
        "dataset (e.g., a dataset where the relationship between input and output is\n",
        "quadratic).\n",
        "\n",
        "Why It Happens: The linear model cannot capture the non-linear relationships in\n",
        "the data, resulting in underfitting.\n",
        "\n",
        "Inadequate Feature Engineering:\n",
        "\n",
        "Example: Using a small number of features or failing to create meaningful features\n",
        " from raw data.\n",
        "\n",
        "Why It Happens: If relevant information is not included, the model has limited\n",
        "input to learn from, making it too simple.\n",
        "\n",
        "Excessive Regularization:\n",
        "\n",
        "Example: Using high values of L1 or L2 regularization.\n",
        "\n",
        "Why It Happens: Excessive regularization can force the model to be too\n",
        "constrained, reducing its ability to learn complex patterns.\n",
        "\n",
        "Insufficient Training Time:\n",
        "\n",
        "Example: Stopping the training process too early or using a small number of\n",
        "training epochs (in deep learning).\n",
        "\n",
        "Why It Happens: The model doesnâ€™t have enough time to fully learn the patterns\n",
        "from the data, leading to poor performance.\n",
        "\n",
        "Using a Very Simple Model for a Complex Problem:\n",
        "\n",
        "Example: Using a decision tree with only a few levels for a problem that requires\n",
        " a deeper or more complex model.\n",
        "\n",
        "Why It Happens: A simple model doesn't have the capacity to capture the\n",
        "complexity of the data.\n",
        "\n",
        "No Hyperparameter Tuning:\n",
        "\n",
        "Example: Using default hyperparameters without adjusting them based on the data.\n",
        "\n",
        "Why It Happens: Default settings may not be optimal for the given data, causing\n",
        " the model to underperform.\n",
        "\n",
        "Mitigating Underfitting:\n",
        "Use more complex models (e.g., deeper neural networks, more complex decision trees).\n",
        "\n",
        "Improve feature engineering to include more relevant features.\n",
        "\n",
        "Reduce regularization strength.\n",
        "\n",
        "Train the model longer (in the case of iterative methods like gradient descent).\n",
        "\n",
        "Tune hyperparameters to find better configurations.\n",
        "\n",
        "4:To reduce bias: Increase model complexity (e.g., using more features or a more\n",
        " flexible model), improve feature engineering, or reduce regularization.\n",
        "\n",
        "To reduce variance: Simplify the model (e.g., use fewer features or reduce model\n",
        " flexibility), gather more data, or apply regularization techniques.\n",
        "\n",
        "5:Detecting whether your model is overfitting or underfitting typically involves\n",
        " analyzing the relationship between training and test errors, examining learning\n",
        "  curves, and adjusting the model's complexity. Once you identify the issue, you\n",
        "   can take the appropriate steps to mitigate it and improve model performance.\n",
        "\n",
        "6:High Bias leads to underfitting: The model is too simple and cannot capture\n",
        "the underlying patterns of the data.\n",
        "\n",
        "High Variance leads to overfitting: The model is too complex and learns the\n",
        "noise in the training data, failing to generalize to new data.\n",
        "\n",
        "Optimal Model: A good model strikes a balance between bias and variance,\n",
        "achieving a low error on both training and test data, which results in good\n",
        "generalization\n",
        "\n",
        "7:Regularization techniques are essential for improving model generalization and\n",
        " preventing overfitting. The choice of regularization depends on the type of\n",
        " model, the problem at hand, and the dataset. Techniques like L1/L2\n",
        " regularization, dropout, and early stopping can help control model complexity\n",
        " and ensure that it performs well on unseen data, not just the training data."
      ]
    }
  ]
}