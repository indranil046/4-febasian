{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU203+1MvhEvQDhwK1vqi9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranil046/4-febasian/blob/main/Regression_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TR7_QAmWZie"
      },
      "outputs": [],
      "source": [
        "1:R-squared (also called the coefficient of determination) is a statistical\n",
        "measure that represents the proportion of the variance in the dependent variable\n",
        " that is explained by the independent variable(s) in a regression model.\n",
        "\n",
        "It tells us how well the regression model fits the data — in other words, how\n",
        "much of the variation in the outcome is accounted for by the predictors.\n",
        "\n",
        "2:Adjusted R-squared is a modified version of R-squared that accounts for the\n",
        "number of predictors (independent variables) in a regression model.\n",
        "\n",
        "It adjusts the R-squared value by penalizing the addition of irrelevant variables,\n",
        " helping to prevent overfitting.\n",
        "\n",
        "Unlike regular R-squared, which can only increase or stay the same when new\n",
        "variables are added, adjusted R-squared can decrease if the added variables do\n",
        "not improve the model sufficiently.\n",
        "\n",
        "3:Multiple Regression Models (More than One Predictor):\n",
        "When your regression model includes multiple independent variables, adjusted\n",
        "R-squared is preferred because it accounts for the number of predictors.\n",
        "\n",
        "4:Use RMSE or MSE when you want to penalize large errors more heavily.\n",
        "\n",
        "Use MAE when you want a measure that treats all errors equally, which is more\n",
        "robust to outliers.\n",
        "\n",
        "5:Use MSE/RMSE when large errors are particularly bad and you want the model to\n",
        " focus on minimizing them.\n",
        "\n",
        "Use MAE when you want a robust measure of typical error, especially if the data\n",
        "contains outliers.\n",
        "\n",
        "\n",
        "6:Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization\n",
        "technique used in linear regression to prevent overfitting by adding a penalty\n",
        "term to the loss function.\n",
        "\n",
        "The penalty is based on the sum of the absolute values of the coefficients\n",
        " (also called L1 regularization).\n",
        "\n",
        "This penalty encourages some coefficients to be exactly zero, effectively\n",
        "performing feature selection by shrinking less important features’ coefficients\n",
        "to zero.\n",
        "\n",
        "7:from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(100, 10)\n",
        "coef = np.array([5, 0, 0, 3, 0, 0, 0, 0, 2, 0])  # Sparse true coefficients\n",
        "y = X @ coef + np.random.randn(100) * 0.5  # Add noise\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Linear Regression (no regularization)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "print(\"Linear Regression MSE:\", mean_squared_error(y_test, y_pred_lr))\n",
        "\n",
        "# Ridge Regression (with regularization)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, y_train)\n",
        "y_pred_ridge = ridge.predict(X_test)\n",
        "print(\"Ridge Regression MSE:\", mean_squared_error(y_test, y_pred_ridge))\n",
        "\n",
        "8:When the relationship between features and target is nonlinear and complex\n",
        " (e.g., image data, time series with seasonality).\n",
        "\n",
        "When interactions between variables are critical but not explicitly modeled.\n",
        "\n",
        "When the dataset contains many categorical variables with many levels\n",
        " (other models like tree-based may handle these better).\n",
        "\n",
        "When you need a model that automatically captures hierarchical or structured\n",
        "\n",
        "relationships, e.g., decision trees, random forests, or neural networks.\n",
        "\n",
        "When interpretability is crucial, but Lasso arbitrarily excludes correlated\n",
        "features, or Ridge keeps too many insignificant ones, causing confusion.\n",
        "\n",
        "9:Metric mismatch: You cannot compare models fairly using different error metrics.\n",
        "\n",
        "No context on data distribution: The presence of outliers or error distribution\n",
        "affects which metric is more meaningful.\n",
        "\n",
        "Single metric limitation: Relying on just one metric can hide aspects of model\n",
        "performance. It's better to consider multiple metrics (RMSE, MAE, R², residual\n",
        "plots).\n",
        "\n",
        "10:You can’t decide solely based on the type of regularization and alpha values\n",
        " without evaluating performance on validation data (like cross-validation RMSE,\n",
        "MAE, or R²). The regularization parameter alpha controls the strength of penalty:\n",
        "\n",
        "Smaller alpha (0.1) means less regularization (Ridge).\n",
        "\n",
        "Larger alpha (0.5) means stronger regularization (Lasso).\n",
        "\n",
        "So, Model B applies stronger shrinkage and potentially sets some coefficients\n",
        "exactly to zero."
      ]
    }
  ]
}